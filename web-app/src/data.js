const Data = {
  id: [
    {
      question:
        "What leaves the model's state at the end of a long sentence without precise, extractable information about preceding tokens.",
      answer: "the vanishing gradient problem",
      context:
        "Before transformers, most state-of-the-art NLP systems relied on gated RNNs, such as LSTM and gated recurrent units (GRUs), with added attention mechanisms. Transformers built on these attention technologies without using an RNN structure, highlighting the fact that attention mechanisms alone can match the performance of RNNs with attention. Gated RNNs process tokens sequentially, maintaining a state vector that contains a representation of the data seen after every token. To process the {\textstyle n}{\textstyle n}th token, the model combines the state representing the sentence up to token {\textstyle n-1}{\textstyle n-1} with the information of the new token to create a new state, representing the sentence up to token {\textstyle n}{\textstyle n}. Theoretically, the information from one token can propagate arbitrarily far down the sequence, if at every point the state continues to encode contextual information about the token. In practice this mechanism is flawed: the vanishing gradient problem leaves the model's state at the end of a long sentence without precise, extractable information about preceding tokens.",
    },
    {
      question: "What are the major components of each encoder?",
      answer: "Self attenion mechanism and feed forward neural network",
      context:
        "Each encoder consists of two major components: a self-attention mechanism and a feed-forward neural network. The self-attention mechanism accepts input encodings from the previous encoder and weighs their relevance to each other to generate output encodings. The feed-forward neural network further processes each output encoding individually. These output encodings are then passed to the next encoder as its input, as well as to the decoders.  The first encoder takes positional information and embeddings of the input sequence as its input, rather than encodings. The positional information is necessary for the transformer to make use of the order of the sequence, because no other part of the transformer makes use of this.[1]",
    },
    {
      question: "How many major components does a decoder have?",
      answer: "Three",
      context:
        "Each decoder consists of three major components: a self-attention mechanism, an attention mechanism over the encodings, and a feed-forward neural network. The decoder functions in a similar fashion to the encoder, but an additional attention mechanism is inserted which instead draws relevant information from the encodings generated by the encoders.[1][7]  Like the first encoder, the first decoder takes positional information and embeddings of the output sequence as its input, rather than encodings. The transformer must not use the current or future output to predict an output, so the output sequence must be partially masked to prevent this reverse information flow.[1] The last decoder is followed by a final linear transformation and softmax layer, to produce the output probabilities over the vocabulary.",
    },
  ],
};

export default Data;
